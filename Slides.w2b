HPC
JCN-9000
%%date(%Y-%m-%d)
%!target: art
%!options(art): --toc --slides --width 80 --height 25
%!target: aat
%!options(aat): --toc --slides --width 80 --height 25
%!target: aap
%!options(aap):       --slides --width 80 --height 25
%!target: aapp
%!options(aapp): --toc --slides --width 80 --height 25
%!target: mgp
%!options(mgp): --toc --slides --width 80 --height 25

%COMMENT
%Run Presentation
%   mgp -t90 -g1280x1024  Slides.mgp

% https://tinyurl.com/
% C-like Defines
%!preproc:  URLHPC https://www.techopedia.com/definition/4595/high-performance-computing-hpc
%!preproc: TINYHPC https://tinyurl.com/yycxbun4
%!preproc:  URLHPCIT https://it.wikipedia.org/wiki/High_Performance_Computing
%!preproc: TINYHPCIT https://tinyurl.com/z3hkcmv
%!preproc:  URLMPI https://en.wikipedia.org/wiki/Message_passing_in_computer_clusters
%!preproc: TINYMPI https://tinyurl.com/6mfo5pf

= Definition =

: [HPC TINYHPC]

    HPC systems tend to focus on tightly coupled parallel jobs, and as
    such they must execute within a particular site with low-latency
    interconnects.

% [images/HPC4-ENI.jpg]
[images/Summit_small.jpg]


= Wikipedia IT =
: [HPC TINYHPCIT]

    Con High Performance Computing (HPC) (in italiano calcolo ad
    elevate prestazioni), in informatica, ci si riferisce alle
    tecnologie utilizzate da computer cluster per creare dei sistemi di
    elaborazione in grado di fornire delle prestazioni molto elevate
    nell'ordine dei PetaFLOPS, ricorrendo tipicamente al calcolo
    parallelo.


= TOP 500 =

[TOP_500 https://www.top500.org/]
[LIST https://www.top500.org/lists/2018/11/]
[1-100 https://www.top500.org/list/2018/11/?page=1]


= IPC - Process =

- shared files / memory + semaphores
- [pipes (named, unnamed) https://opensource.com/article/19/4/interprocess-communication-linux-channels]
- message queues                unidirectional
- sockets (memory, network )    bi-directional
- signals
- [RPC https://en.wikipedia.org/wiki/Remote_procedure_call]
  - ONC/RPC, XML-RPC -> SOAP, CORBA, JSON-RPC, gRPC, ...


= Approaches to message passing =
: PVM
    Parallel Virtual Machine
    [PVM https://en.wikipedia.org/wiki/Parallel_Virtual_Machine]
: MPI the Message Passing Interface
    [MPI TINYMPI]


= MPI =

--map-by hwthread
--rank-by hwthread
--bind-to hwthread
--use-hwthread-cpus

mpirun --use-hwthread-cpus --bind-to hwthread -np 4

mpirun --map-by node    : balanced - rrobin
mpirun -nolocal




mpirun --use-hwthread-cpus --bind-to hwthread -np 1 search_mpi
  Elapsed wallclock time is 59.731
mpirun --use-hwthread-cpus --bind-to hwthread -np 2 search_mpi
  Elapsed wallclock time is 44.0702
mpirun --use-hwthread-cpus --bind-to hwthread -np 3 search_mpi
  Elapsed wallclock time is 31.4417


= Pelican HPC =


https://www.pelicanhpc.org/index.html


= PelicanHPC over Virtualbox =

First Steps ...
sudo mkdir /cdrom
sudo mount -oro /dev/sr1 /cdrom
sudo /cdrom/VBoxLinuxAdditions.run

sudo ln -sf /home/etc/apt/sources.list /etc/apt/sources.list
sudo apt update
sudo apt install libatlas3-base
sudo apt install gpm

cd hpl-2.0
sh SetupForPelican
cd bin/Pelican HPC
orterun --hostfile /home/usr/tmp/bhosts -np 4 xhpl



= PelicanHPC over KVM =


= URLS =

[HPC URLHPC]



% vim: tw=55

===========================================================================================
/home/avaresio/GIT/txt2tags/txt2tags -t aapp --slides Slides.t2t
enscript -l -r -p file.ps Slides.aapp
ps2pdf file.ps Slides.pdf
impressive -f -g 1024x768 Slides.pdf



