<!DOCTYPE html>
<html>
<head>
<title>HPC</title>
<meta name="generator" content="http://txt2tags.org">
<style>
body{background-color:#fff;color:#000;}
hr{background-color:#000;border:0;color:#000;}
hr.heavy{height:5px;}
hr.light{height:1px;}
img{border:0;display:block;}
img.right{margin:0 0 0 auto;}
img.center{border:0;margin:0 auto;}
table th,table td{padding:4px;}
.center,header{text-align:center;}
table.center {margin-left:auto; margin-right:auto;}
.right{text-align:right;}
.left{text-align:left;}
.tableborder,.tableborder td,.tableborder th{border:1px solid #000;}
.underline{text-decoration:underline;}
</style>
</head>
<body>
<header>
<hgroup>
<h1>HPC</h1>
<h2>JCN-9000</h2>
<h3>2019-05-09</h3>
</hgroup>
</header>
<article>


<section>
<h1>Definition</h1>

<dl>
<dt><a href="https://tinyurl.com/yycxbun4">HPC</a></dt><dd>
<p></p>
    HPC systems tend to focus on tightly coupled parallel jobs, and as
    such they must execute within a particular site with low-latency
    interconnects.
<p></p>
<img class="center" src="images/Summit_small.jpg" alt="">
</dd>
</dl>

</section>
<section>
<h1>Wikipedia IT</h1>

<dl>
<dt><a href="https://tinyurl.com/yycxbun4IT">HPC</a></dt><dd>
<p></p>
    Con High Performance Computing (HPC) (in italiano calcolo ad
    elevate prestazioni), in informatica, ci si riferisce alle
    tecnologie utilizzate da computer cluster per creare dei sistemi di
    elaborazione in grado di fornire delle prestazioni molto elevate
    nell'ordine dei PetaFLOPS, ricorrendo tipicamente al calcolo
    parallelo.
</dd>
</dl>

</section>
<section>
<h1>TOP 500</h1>

<p>
<a href="https://www.top500.org/">TOP_500</a>
<a href="https://www.top500.org/lists/2018/11/">LIST</a>
<a href="https://www.top500.org/list/2018/11/?page=1">1-100</a>
</p>

</section>
<section>
<h1>IPC - Process</h1>

<ul>
<li>shared files / memory + semaphores
</li>
<li><a href="https://opensource.com/article/19/4/interprocess-communication-linux-channels">pipes (named, unnamed)</a>
</li>
<li>message queues                unidirectional
</li>
<li>sockets (memory, network )    bi-directional
</li>
<li>signals
</li>
<li><a href="https://en.wikipedia.org/wiki/Remote_procedure_call">RPC</a>
  <ul>
  <li>ONC/RPC, XML-RPC -&gt; SOAP, CORBA, JSON-RPC, gRPC, ...
  </li>
  </ul>
</li>
</ul>

</section>
<section>
<h1>Approaches to message passing</h1>

<dl>
<dt>PVM</dt><dd>
    Parallel Virtual Machine
    <a href="https://en.wikipedia.org/wiki/Parallel_Virtual_Machine">PVM</a>
</dd>
<dt>MPI the Message Passing Interface</dt><dd>
    <a href="https://tinyurl.com/6mfo5pf">MPI</a>
</dd>
</dl>

</section>
<section>
<h1>MPI</h1>

<p>
--map-by hwthread
--rank-by hwthread
--bind-to hwthread
--use-hwthread-cpus
</p>
<p>
mpirun --use-hwthread-cpus --bind-to hwthread -np 4
</p>
<p>
mpirun --map-by node    : balanced - rrobin
mpirun -nolocal
</p>
<p>
mpirun --use-hwthread-cpus --bind-to hwthread -np 1 search_mpi
  Elapsed wallclock time is 59.731
mpirun --use-hwthread-cpus --bind-to hwthread -np 2 search_mpi
  Elapsed wallclock time is 44.0702
mpirun --use-hwthread-cpus --bind-to hwthread -np 3 search_mpi
  Elapsed wallclock time is 31.4417
</p>

</section>
<section>
<h1>Pelican HPC</h1>

<p>
<a href="https://www.pelicanhpc.org/index.html">https://www.pelicanhpc.org/index.html</a>
</p>

</section>
<section>
<h1>PelicanHPC over Virtualbox</h1>

<p>
First Steps ...
sudo mkdir /cdrom
sudo mount -oro /dev/sr1 /cdrom
sudo /cdrom/VBoxLinuxAdditions.run
</p>
<p>
sudo ln -sf /home/etc/apt/sources.list /etc/apt/sources.list
sudo apt update
sudo apt install libatlas3-base
sudo apt install gpm
</p>
<p>
cd hpl-2.0
sh SetupForPelican
cd bin/Pelican HPC
orterun --hostfile /home/usr/tmp/bhosts -np 4 xhpl
</p>

</section>
<section>
<h1>PelicanHPC over KVM</h1>

</section>
<section>
<h1>URLS</h1>

<p>
<a href="https://www.techopedia.com/definition/4595/high-performance-computing-hpc">HPC</a>
</p>
</section>

<!-- html5 code generated by txt2tags 2.6. (http://txt2tags.org) -->
<!-- cmdline: txt2tags -t html5 Slides.t2t -->
</article></body></html>
