<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"> 
<head> 
  <title>HPC</title> 
  <meta name="copyright" 
   content="Copyright &#169; JCN-9000 JCN-9000" /> 
  <meta name="generator" content="http://txt2tags.org" />
  <link rel="stylesheet" type="text/css" media="screen, projection, print" href="slidy_t2t.css" /> 
  <script src="http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js.gz" charset="utf-8" type="text/javascript"></script> 
  <script src="slidy.js" charset="utf-8" type="text/javascript"></script> 
</head>

<div class="background"><a href="http://www.txt2tags.org/"><img
alt="txt2tags logo" id="head-logo-fallback"
src="t2tgems.png" align="right"/></a>
</div>


<h1>Definition</h1>

<dl>
<dt><a href="https://tinyurl.com/yycxbun4">HPC</a></dt><dd>
<p></p>
    HPC systems tend to focus on tightly coupled parallel jobs, and as
    such they must execute within a particular site with low-latency
    interconnects.
<p></p>
<img align="middle" src="images/Summit_small.jpg" border="0" alt=""/>
</dd>
</dl>

<h1>Wikipedia IT</h1>

<dl>
<dt><a href="https://tinyurl.com/yycxbun4IT">HPC</a></dt><dd>
<p></p>
    Con High Performance Computing (HPC) (in italiano calcolo ad
    elevate prestazioni), in informatica, ci si riferisce alle
    tecnologie utilizzate da computer cluster per creare dei sistemi di
    elaborazione in grado di fornire delle prestazioni molto elevate
    nell'ordine dei PetaFLOPS, ricorrendo tipicamente al calcolo
    parallelo.
</dd>
</dl>

<h1>TOP 500</h1>

<ul>
<li><a href="https://www.top500.org/">TOP_500</a>
</li>
<li><a href="https://www.top500.org/lists/2018/11/">LIST</a>
</li>
<li><a href="https://www.top500.org/list/2018/11/?page=1">1-100</a>
</li>
</ul>

<h1>IPC - Process</h1>

<ul>
<li>shared files / memory + semaphores
</li>
<li><a href="https://opensource.com/article/19/4/interprocess-communication-linux-channels">pipes (named, unnamed)</a>
</li>
<li>message queues                unidirectional
</li>
<li>sockets (memory, network )    bi-directional
</li>
<li>signals
</li>
<li><a href="https://en.wikipedia.org/wiki/Remote_procedure_call">RPC</a>
  <ul>
  <li>ONC/RPC, XML-RPC -&gt; SOAP, CORBA, JSON-RPC, gRPC, ...
  </li>
  </ul>
</li>
</ul>

<h1>Approaches to message passing</h1>

<dl>
<dt>PVM</dt><dd>
    Parallel Virtual Machine
    <a href="https://en.wikipedia.org/wiki/Parallel_Virtual_Machine">PVM</a>
</dd>
<dt>MPI the Message Passing Interface</dt><dd>
    <a href="https://tinyurl.com/6mfo5pf">MPI</a>
</dd>
</dl>

<h1>MPI</h1>

<p>
--map-by hwthread
--rank-by hwthread
--bind-to hwthread
--use-hwthread-cpus
</p>
<p>
mpirun --use-hwthread-cpus --bind-to hwthread -np 4
</p>
<p>
mpirun --map-by node    : balanced - rrobin
mpirun -nolocal
</p>
<p>
mpirun --use-hwthread-cpus --bind-to hwthread -np 1 search_mpi
  Elapsed wallclock time is 59.731
mpirun --use-hwthread-cpus --bind-to hwthread -np 2 search_mpi
  Elapsed wallclock time is 44.0702
mpirun --use-hwthread-cpus --bind-to hwthread -np 3 search_mpi
  Elapsed wallclock time is 31.4417
</p>

<h1>Pelican HPC</h1>

<p>
<a href="https://www.pelicanhpc.org/index.html">https://www.pelicanhpc.org/index.html</a>
</p>

<h1>PelicanHPC over Virtualbox</h1>

<p>
First Steps ...
sudo mkdir /cdrom
sudo mount -oro /dev/sr1 /cdrom
sudo /cdrom/VBoxLinuxAdditions.run
</p>
<p>
sudo ln -sf /home/etc/apt/sources.list /etc/apt/sources.list
sudo apt update
sudo apt install libatlas3-base
sudo apt install gpm
</p>
<p>
cd hpl-2.0
sh SetupForPelican
cd bin/Pelican HPC
orterun --hostfile /home/usr/tmp/bhosts -np 4 xhpl
</p>

<h1>PelicanHPC over KVM</h1>

<h1>URLS</h1>

<ul>
<li><a href="https://www.techopedia.com/definition/4595/high-performance-computing-hpc">HPC</a>
</li>
</ul>

<h1>IDEAS</h1>

<p>
FLOPS Unit Peta Exa  <a href="https://en.m.wikipedia.org/wiki/Unit_prefix">https://en.m.wikipedia.org/wiki/Unit_prefix</a>
Q commands
Queuing system
Hardware components
Power Supply
CPU
GPU
Network
Infiniband
Intel OmniPath
Fast Ethernet
Shared Filesystem
NFS
Lustre Gluster BeeGFS
MP. Hibrid: threads + process
OpenMPI - OpenMP
PlatformMPI
IntelMPI
<a href="https://github.com/wiki2beamer/wiki2beamer">https://github.com/wiki2beamer/wiki2beamer</a>
<a href="https://openhpc.community">https://openhpc.community</a>
</p>

<h1>EOD</h1>

<hr class="heavy" />

<p>
EOD
</p>

<hr class="heavy" />

<p>
/home/avaresio/GIT/txt2tags/txt2tags -t aapp --slides Slides.t2t
enscript -l -r -p file.ps Slides.aapp
ps2pdf file.ps Slides.pdf
</p>
<p>
/home/avaresio/GIT/txt2tags/txt2tags -t txt2t Slides.t2t
pandoc -s -t beamer -o Slides.tex Slides.txt2t
pdflatex Slides.tex &gt; Slides.pdf
rm Slides.out Slides.snm Slides.toc Slides.log Slides.vrb Slides.nav Slides.aux
bl-file-manager Slides.pdf
</p>
<p>
impressive -f -g 1024x768 Slides.pdf
</p>

<!-- slidy code generated by txt2tags 2.6. (http://txt2tags.org) -->
<!-- cmdline: txt2tags -t slidy Slides.t2t -->
</body></html>
